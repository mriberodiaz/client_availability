{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import collections\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "from utils import training_loop\n",
    "from utils import training_loop_importance\n",
    "from utils import training_utils\n",
    "from utils.datasets import emnist_dataset\n",
    "from utils.models import emnist_models\n",
    "\n",
    "\n",
    "from optimization.shared import importance_gradient_schedule as importance_schedule\n",
    "from optimization.shared import importance_aggregation_factory\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "client_batch_size = 20\n",
    "client_epochs_per_round=1\n",
    "max_batches_per_client=100\n",
    "clients_per_round = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train, _ = emnist_dataset.get_emnist_datasets(\n",
    "  client_batch_size,\n",
    "  client_epochs_per_round,\n",
    "  max_batches_per_client=max_batches_per_client,\n",
    "  only_digits=False)\n",
    "\n",
    "input_spec = emnist_train.create_tf_dataset_for_client(\n",
    "  emnist_train.client_ids[0]).element_spec\n",
    "\n",
    "model_builder = functools.partial(\n",
    "        emnist_models.create_two_hidden_layer_model, only_digits=False)\n",
    "\n",
    "loss_builder = tf.keras.losses.SparseCategoricalCrossentropy\n",
    "metrics_builder = lambda: [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "def tff_model_fn() -> tff.learning.Model:\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=model_builder(),\n",
    "        input_spec=input_spec,\n",
    "        loss=loss_builder(),\n",
    "        metrics=metrics_builder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_optimizer_fn = tf.keras.optimizers.SGD\n",
    "server_optimizer_fn = tf.keras.optimizers.SGD\n",
    "\n",
    "client_lr_schedule = lambda _: 0.1\n",
    "server_lr_schedule = lambda _: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_lr_schedule = lambda _: 0.1\n",
    "server_lr_schedule = lambda _: 0.1\n",
    "def iterative_process_builder(\n",
    "    model_fn: Callable[[], tff.learning.Model],\n",
    "    client_weight_fn: Optional[Callable[[Any], tf.Tensor]] = None,\n",
    ") -> tff.templates.IterativeProcess:\n",
    "\n",
    "    factory = importance_aggregation_factory.ImportanceSamplingFactory(clients_per_round)\n",
    "    weights_type = importance_aggregation_factory.weights_type_from_model_fn(model_fn)\n",
    "    importance_aggregation_process = factory.create(\n",
    "    value_type = weights_type,\n",
    "    weight_type = tff.TensorType(tf.float32))\n",
    "\n",
    "    return importance_schedule.build_fed_avg_process(\n",
    "      model_fn=model_fn,\n",
    "      client_optimizer_fn=client_optimizer_fn,\n",
    "      client_lr=client_lr_schedule,\n",
    "      server_optimizer_fn=server_optimizer_fn,\n",
    "      server_lr=server_lr_schedule,\n",
    "      aggregation_process = importance_aggregation_process)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_process = iterative_process_builder(model_fn = tff_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets_fn = training_utils.build_availability_client_datasets_fn(\n",
    "      train_dataset = emnist_train, \n",
    "      train_clients_per_round = clients_per_round, \n",
    "      beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_data, federated_weights,r_vec,idx_ids,avail = client_datasets_fn(round_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = training_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2, round_metrics = training_process.next(initial_state,\n",
    "                                              federated_train_data, federated_weights.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.047363598"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state.model.trainable[0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04736358"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state1.model.trainable[0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([0.00481723, 0.00312181, 0.00397017, 0.00509929, 0.00538122,\n",
       "        0.00509929, 0.00397017, 0.00481723, 0.00340474, 0.00425267],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(3400,) dtype=float32, numpy=\n",
       " array([0.00046105, 0.00048667, 0.00040982, ..., 0.0001793 , 0.0001793 ,\n",
       "        0.00020491], dtype=float32)>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federated_weights, r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.004/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=185.83156>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1/max(federated_weights/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1560905"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.00312181*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09634459999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.00481723*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.008403361344538"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/(0.99*6+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3400,) dtype=float32, numpy=\n",
       "array([0.00046105, 0.00048667, 0.00040982, ..., 0.0001793 , 0.0001793 ,\n",
       "       0.00020491], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00963530303030303"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00046105-0.01)/0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr54725/tff/lib/python3.8/site-packages/tensorflow/python/util/nest.py:764: RuntimeWarning: coroutine 'wrap_coroutine_in_current_trace_context.<locals>._wrapped' was never awaited\n",
      "  input_tree = dict(_yield_sorted_items(input_tree))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/mr54725/tff/lib/python3.8/site-packages/tensorflow/python/util/nest.py:764: RuntimeWarning: coroutine '_invoke' was never awaited\n",
      "  input_tree = dict(_yield_sorted_items(input_tree))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-446f9ded42b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memnist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tf_dataset_for_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mp_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "p_vector = [ ]\n",
    "for client_id in emnist_train.client_ids:\n",
    "    dataset = emnist_train.create_tf_dataset_for_client(client_id)\n",
    "    p_vector.append(len(list(dataset)))\n",
    "p_vector = np.array(p_vector)/sum(p_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vector = np.array(p_vector)/sum(p_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00051228, 0.00054074, 0.00045536, ..., 0.00019922, 0.00019922,\n",
       "       0.00022768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048755337932890164"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.00051228/(0.99*0.00051228+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([0.00481723, 0.00312181, 0.00397017, 0.00509929, 0.00538122,\n",
       "       0.00509929, 0.00397017, 0.00481723, 0.00340474, 0.00425267],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 47,  81,  84,  90,  92, 145, 255, 328, 401, 406], dtype=int32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004817228676678945"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vector[47]/(0.9*p_vector[47]+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_k: 0.0024828528331957765  -  pk/rk = 0.19486474094452086\n",
      "r_k: 0.0044778871275293845  -  pk/rk = 0.10804659884560473\n",
      "r_k: 0.006468931353274325  -  pk/rk = 0.07479140645064555\n",
      "r_k: 0.008455993490567776  -  pk/rk = 0.05721627797889644\n",
      "r_k: 0.01043908150358664  -  pk/rk = 0.04634703483997462\n",
      "r_k: 0.012418203340579467  -  pk/rk = 0.03896058559156177\n",
      "r_k: 0.014393366933898307  -  pk/rk = 0.033614127699656056\n",
      "r_k: 0.01636458020003051  -  pk/rk = 0.029565101470990538\n",
      "r_k: 0.018331851039630448  -  pk/rk = 0.02639234156431472\n",
      "r_k: 0.020295187337551188  -  pk/rk = 0.02383917261255704\n",
      "r_k: 0.022254596962876086  -  pk/rk = 0.021740248765284215\n",
      "r_k: 0.024210087768950332  -  pk/rk = 0.019984251141979294\n",
      "r_k: 0.02616166759341243  -  pk/rk = 0.018493487558334843\n",
      "r_k: 0.028109344258225602  -  pk/rk = 0.017212086831320687\n",
      "r_k: 0.030053125569709148  -  pk/rk = 0.016098840469082932\n",
      "r_k: 0.03199301931856973  -  pk/rk = 0.01512268877552424\n",
      "r_k: 0.033929033279932595  -  pk/rk = 0.014259777758838221\n",
      "r_k: 0.03586117521337273  -  pk/rk = 0.013491484070595841\n",
      "r_k: 0.03778945286294599  -  pk/rk = 0.012803055812921528\n",
      "r_k: 0.0397138739572201  -  pk/rk = 0.012182656234071687\n",
      "r_k: 0.04163444620930566  -  pk/rk = 0.011620677544545472\n",
      "r_k: 0.043551177316887055  -  pk/rk = 0.011109239840376537\n",
      "r_k: 0.045464074962253284  -  pk/rk = 0.010641819382573128\n",
      "r_k: 0.04737314681232878  -  pk/rk = 0.0102129688800439\n",
      "r_k: 0.04927840051870412  -  pk/rk = 0.009818104261732798\n",
      "r_k: 0.05117984371766671  -  pk/rk = 0.009453340201917326\n",
      "r_k: 0.05307748403023138  -  pk/rk = 0.009115361871118357\n",
      "r_k: 0.05497132906217092  -  pk/rk = 0.008801323933734225\n",
      "r_k: 0.05686138640404658  -  pk/rk = 0.008508770269970645\n",
      "r_k: 0.05874766363123849  -  pk/rk = 0.0082355696250497\n",
      "r_k: 0.060630168303976016  -  pk/rk = 0.007979863617042551\n",
      "r_k: 0.06250890796736806  -  pk/rk = 0.007740024420145632\n",
      "r_k: 0.06438389015143332  -  pk/rk = 0.00751462008595785\n",
      "r_k: 0.06625512237113046  -  pk/rk = 0.007302385941330344\n",
      "r_k: 0.0681226121263882  -  pk/rk = 0.007102200855810259\n",
      "r_k: 0.06998636690213543  -  pk/rk = 0.006913067438128472\n",
      "r_k: 0.07184639416833116  -  pk/rk = 0.00673409542322342\n",
      "r_k: 0.0737027013799945  -  pk/rk = 0.006564487665785755\n",
      "r_k: 0.07555529597723451  -  pk/rk = 0.006403528275368601\n",
      "r_k: 0.07740418538528004  -  pk/rk = 0.006250572520540638\n",
      "r_k: 0.07924937701450949  -  pk/rk = 0.006105038201820611\n",
      "r_k: 0.08109087826048048  -  pk/rk = 0.005966398249997174\n",
      "r_k: 0.08292869650395952  -  pk/rk = 0.005834174351467879\n",
      "r_k: 0.0847628391109516  -  pk/rk = 0.005707931438100611\n",
      "r_k: 0.0865933134327297  -  pk/rk = 0.00558727290785474\n",
      "r_k: 0.08842012680586425  -  pk/rk = 0.005471836465540744\n",
      "r_k: 0.09024328655225253  -  pk/rk = 0.0053612904918298124\n",
      "r_k: 0.09206279997914803  -  pk/rk = 0.005255330863863023\n",
      "r_k: 0.09387867437918973  -  pk/rk = 0.005153678163262541\n",
      "r_k: 0.09569091703043135  -  pk/rk = 0.0050560752175695155\n",
      "r_k: 0.09749953519637049  -  pk/rk = 0.0049622849295601085\n",
      "r_k: 0.09930453612597775  -  pk/rk = 0.00487208835586614\n",
      "r_k: 0.1011059270537258  -  pk/rk = 0.004785283002122828\n",
      "r_k: 0.10290371519961836  -  pk/rk = 0.004701681306700373\n",
      "r_k: 0.10469790776921913  -  pk/rk = 0.004621109289122838\n",
      "r_k: 0.10648851195368068  -  pk/rk = 0.00454340534267689\n",
      "r_k: 0.10827553492977332  -  pk/rk = 0.004468419153577647\n",
      "r_k: 0.11005898385991378  -  pk/rk = 0.004396010731480905\n",
      "r_k: 0.11183886589219395  -  pk/rk = 0.004326049538185044\n",
      "r_k: 0.11361518816040957  -  pk/rk = 0.004258413703113129\n",
      "r_k: 0.11538795778408875  -  pk/rk = 0.004192989315656129\n",
      "r_k: 0.11715718186852057  -  pk/rk = 0.004129669785733078\n",
      "r_k: 0.11892286750478354  -  pk/rk = 0.0040683552650175\n",
      "r_k: 0.12068502176977397  -  pk/rk = 0.004008952122219688\n",
      "r_k: 0.12244365172623442  -  pk/rk = 0.003951372466625011\n",
      "r_k: 0.12419876442278195  -  pk/rk = 0.003895533714788847\n",
      "r_k: 0.12595036689393638  -  pk/rk = 0.003841358195895475\n",
      "r_k: 0.12769846616014852  -  pk/rk = 0.003788772791814886\n",
      "r_k: 0.12944306922782822  -  pk/rk = 0.0037377086083497384\n",
      "r_k: 0.13118418308937257  -  pk/rk = 0.0036881006745641707\n",
      "r_k: 0.13292181472319384  -  pk/rk = 0.003639887667435236\n",
      "r_k: 0.13465597109374744  -  pk/rk = 0.0035930116593732705\n",
      "r_k: 0.13638665915155995  -  pk/rk = 0.0035474178864255204\n",
      "r_k: 0.13811388583325684  -  pk/rk = 0.0035030545352128825\n",
      "r_k: 0.13983765806159032  -  pk/rk = 0.003459872546856942\n",
      "r_k: 0.14155798274546713  -  pk/rk = 0.003417825436337374\n",
      "r_k: 0.1432748667799762  -  pk/rk = 0.003376869125881347\n",
      "r_k: 0.14498831704641624  -  pk/rk = 0.0033369617911295254\n",
      "r_k: 0.1466983404123234  -  pk/rk = 0.003298063718950029\n",
      "r_k: 0.14840494373149876  -  pk/rk = 0.003260137175884218\n",
      "r_k: 0.15010813384403576  -  pk/rk = 0.0032231462863082437\n",
      "r_k: 0.1518079175763477  -  pk/rk = 0.0031870569194834\n",
      "r_k: 0.153504301741195  -  pk/rk = 0.003151836584747805\n",
      "r_k: 0.1551972931377126  -  pk/rk = 0.003117454334172903\n",
      "r_k: 0.15688689855143717  -  pk/rk = 0.003083880672071789\n",
      "r_k: 0.1585731247543343  -  pk/rk = 0.003051087470803216\n",
      "r_k: 0.16025597850482562  -  pk/rk = 0.003019047892366124\n",
      "r_k: 0.16193546654781596  -  pk/rk = 0.0029877363153253595\n",
      "r_k: 0.16361159561472033  -  pk/rk = 0.0029571282666504034\n",
      "r_k: 0.1652843724234909  -  pk/rk = 0.002927200358086014\n",
      "r_k: 0.1669538036786439  -  pk/rk = 0.0028979302267070966\n",
      "r_k: 0.1686198960712866  -  pk/rk = 0.0028692964793402688\n",
      "r_k: 0.17028265627914405  -  pk/rk = 0.0028412786405618354\n",
      "r_k: 0.17194209096658575  -  pk/rk = 0.002813857104006532\n",
      "r_k: 0.17359820678465257  -  pk/rk = 0.002787013086743694\n",
      "r_k: 0.17525101037108326  -  pk/rk = 0.0027607285864977584\n",
      "r_k: 0.1769005083503411  -  pk/rk = 0.002734986341508338\n",
      "r_k: 0.17854670733364042  -  pk/rk = 0.002709769792841802\n",
      "r_k: 0.18018961391897315  -  pk/rk = 0.0026850630489814293\n",
      "r_k: 0.1818292346911352  -  pk/rk = 0.00266085085253704\n"
     ]
    }
   ],
   "source": [
    "beta = 0.002\n",
    "rk = p_vector[47]\n",
    "\n",
    "for i in range(100):\n",
    "    rk = ((1-beta)*rk+beta)\n",
    "    print(f'r_k: {rk}  -  pk/rk = {p_vector[47]/rk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1001/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001998001998001998"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER=5\n",
    "samples_per_user = 100*np.random.lognormal(4, 2, (NUM_USER)).astype(int) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4850,  3650,  4650,  2150, 11950])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.63119485, -0.1506957 , -0.61193213,  2.1335591 , -0.2654838 ,\n",
       "       -0.45610663, -0.78061412, -0.4595196 ,  0.13269612,  1.48245633])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.random.normal(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction = {'a':[1,2,3], 'b' : [4,5],'train user' : np.array([1,2,3]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dictio.npy', diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "holita = np.load('dictio.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-4baa51d14d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mholita\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "holita['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.random.multivariate_normal(mean = np.zeros(10), cov = np.eye(10),size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 10)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[:75].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "from utils import test\n",
    "from utils.datasets import synthetic_dataset\n",
    "from utils import training_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, fed_test_data = synthetic_dataset.generate_federated_softmax_data(\n",
    "    batch_size = 20, \n",
    "    client_epochs_per_round= 1,\n",
    "    test_batch_size = 100,\n",
    "    alpha=1,\n",
    "    beta=1,\n",
    "    iid = True, \n",
    "    num_users=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logistic_regression_model():\n",
    "  \"\"\"Logistic regression model.\n",
    "\n",
    "  Returns:\n",
    "    An uncompiled `tf.keras.Model`.\n",
    "  \"\"\"\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "        10, \n",
    "        input_shape = (60,), \n",
    "        ),\n",
    "      ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_spec = train_data.create_tf_dataset_for_client(\n",
    "  train_data.client_ids[0]).element_spec\n",
    "\n",
    "\n",
    "model_builder = functools.partial(\n",
    "    create_logistic_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_builder = tf.keras.losses.SparseCategoricalCrossentropy\n",
    "metrics_builder = lambda: [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "def tff_model_fn() -> tff.learning.Model:\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=model_builder(),\n",
    "        input_spec=input_spec,\n",
    "        loss=loss_builder(),\n",
    "        metrics=metrics_builder())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_fn = training_utils.build_evaluate_fn(\n",
    "      eval_dataset=test_data,\n",
    "      model_builder=model_builder,\n",
    "      loss_builder=loss_builder,\n",
    "      metrics_builder=metrics_builder)\n",
    "test_fn = training_utils.build_unweighted_test_fn(\n",
    "      federated_eval_dataset=fed_test_data,\n",
    "      model_builder=model_builder,\n",
    "      loss_builder=loss_builder,\n",
    "      metrics_builder=metrics_builder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setup_outputs(root_output_dir,\n",
    "                   experiment_name,\n",
    "                   hparam_dict,\n",
    "                   write_metrics_with_bz2=True,\n",
    "                   rounds_per_profile=0):\n",
    "  \"\"\"Set up directories for experiment loops, write hyperparameters to disk.\"\"\"\n",
    "\n",
    "  if not experiment_name:\n",
    "    raise ValueError('experiment_name must be specified.')\n",
    "\n",
    "  create_if_not_exists(root_output_dir)\n",
    "\n",
    "  checkpoint_dir = os.path.join(root_output_dir, 'checkpoints', experiment_name)\n",
    "  create_if_not_exists(checkpoint_dir)\n",
    "  checkpoint_mngr = checkpoint_manager.FileCheckpointManager(checkpoint_dir)\n",
    "\n",
    "  results_dir = os.path.join(root_output_dir, 'results', experiment_name)\n",
    "  create_if_not_exists(results_dir)\n",
    "  metrics_mngr = metrics_manager.ScalarMetricsManager(\n",
    "      results_dir, use_bz2=write_metrics_with_bz2)\n",
    "\n",
    "  summary_logdir = os.path.join(root_output_dir, 'logdir', experiment_name)\n",
    "  create_if_not_exists(summary_logdir)\n",
    "  summary_writer = tf.summary.create_file_writer(summary_logdir)\n",
    "\n",
    "  if hparam_dict:\n",
    "    hparam_dict['metrics_file'] = metrics_mngr.metrics_filename\n",
    "    hparams_file = os.path.join(results_dir, 'hparams.csv')\n",
    "    utils_impl.atomic_write_to_csv(pd.Series(hparam_dict), hparams_file)\n",
    "\n",
    "  logging.info('Writing...')\n",
    "  logging.info('    checkpoints to: %s', checkpoint_dir)\n",
    "  logging.info('    metrics csv to: %s', metrics_mngr.metrics_filename)\n",
    "  logging.info('    summaries to: %s', summary_logdir)\n",
    "\n",
    "  @contextlib.contextmanager\n",
    "  def profiler(round_num):\n",
    "    if (rounds_per_profile > 0 and round_num % rounds_per_profile == 0):\n",
    "      with tf.profiler.experimental.Profile(summary_logdir):\n",
    "        yield\n",
    "    else:\n",
    "      yield\n",
    "\n",
    "  return checkpoint_mngr, metrics_mngr, summary_writer, profiler\n",
    "\n",
    "\n",
    "def _write_metrics(metrics_mngr, summary_writer, metrics, round_num):\n",
    "  \"\"\"Atomic metrics writer which inlines logic from MetricsHook class.\"\"\"\n",
    "  if not isinstance(metrics, dict):\n",
    "    raise TypeError('metrics should be type `dict`.')\n",
    "  if not isinstance(round_num, int):\n",
    "    raise TypeError('round_num should be type `int`.')\n",
    "\n",
    "  flat_metrics = metrics_mngr.update_metrics(round_num, metrics)\n",
    "  logging.info('Evaluation at round {:d}:\\n{!s}'.format(\n",
    "      round_num, pprint.pformat(flat_metrics)))\n",
    "\n",
    "  # Also write metrics to a tf.summary logdir\n",
    "  with summary_writer.as_default():\n",
    "    for name, val in flat_metrics.items():\n",
    "      tf.summary.scalar(name, val, step=round_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tff",
   "language": "python",
   "name": "tff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
